{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "\n",
    "class TransformerGenerator(nn.Module):\n",
    "    def __init__(self, d_noise, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerGenerator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(d_noise, d_model)\n",
    "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, 28*28)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = self.embedding(z)\n",
    "        z = z.unsqueeze(0)  # Add batch dimension\n",
    "        transformer_out = self.transformer(z, z)\n",
    "        transformer_out = transformer_out.squeeze(0)  # Remove batch dimension\n",
    "        img = self.output_layer(transformer_out)\n",
    "        img = self.tanh(img)\n",
    "        img = img.view(-1, 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "d_noise = 100\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = TransformerGenerator(d_noise, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "batch_size = 64\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train Discriminator\n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    # Sample noise as generator input\n",
    "    z = torch.randn(batch_size, d_noise).to(device)\n",
    "    \n",
    "    # Generate a batch of images\n",
    "    gen_imgs = generator(z)\n",
    "    \n",
    "    # Generate a batch of real images\n",
    "    real_imgs = torch.randn(batch_size, 1, 28, 28).to(device)  # Replace with real images from your dataset\n",
    "    \n",
    "    # Adversarial ground truths\n",
    "    valid = torch.ones(batch_size, 1).to(device)\n",
    "    fake = torch.zeros(batch_size, 1).to(device)\n",
    "    \n",
    "    # Loss for real images\n",
    "    real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "    \n",
    "    # Loss for fake images\n",
    "    fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "    \n",
    "    # Total discriminator loss\n",
    "    d_loss = (real_loss + fake_loss) / 2\n",
    "    \n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    # Train Generator\n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    # Loss for fake images\n",
    "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "    \n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} - D loss: {d_loss.item()} - G loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDiscriminator(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerDiscriminator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(28*28, d_model)\n",
    "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, 0, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        img_emb = self.embedding(img_flat)\n",
    "        img_emb = img_emb.unsqueeze(0)  # Add batch dimension\n",
    "        transformer_out = self.transformer(img_emb, img_emb)\n",
    "        transformer_out = transformer_out.squeeze(0)  # Remove batch dimension\n",
    "        validity = self.output_layer(transformer_out)\n",
    "        validity = self.sigmoid(validity)\n",
    "        return validity\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(d_noise, d_hidden).to(device)\n",
    "discriminator = TransformerDiscriminator(d_model, nhead, num_encoder_layers, dim_feedforward, dropout).to(device)\n",
    "\n",
    "# The rest of the training loop remains the same\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
