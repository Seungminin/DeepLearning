{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "is_cuda = torch.cuda.is_available() #GPU 실행 (true, false)\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "\n",
    "class TransformerGenerator(nn.Module):\n",
    "    def __init__(self, d_noise, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerGenerator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(d_noise, d_model)\n",
    "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, 28*28)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = self.embedding(z)\n",
    "        z = z.unsqueeze(0)  # Add batch dimension\n",
    "        transformer_out = self.transformer(z, z)\n",
    "        transformer_out = transformer_out.squeeze(0)  # Remove batch dimension\n",
    "        img = self.output_layer(transformer_out)\n",
    "        img = self.tanh(img)\n",
    "        img = img.view(-1, 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "class TransformerDiscriminator(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerDiscriminator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(28*28, d_model)\n",
    "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, 0, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        img_emb = self.embedding(img_flat)\n",
    "        img_emb = img_emb.unsqueeze(0)  # Add batch dimension\n",
    "        transformer_out = self.transformer(img_emb, img_emb)\n",
    "        transformer_out = transformer_out.squeeze(0)  # Remove batch dimension\n",
    "        validity = self.output_layer(transformer_out)\n",
    "        validity = self.sigmoid(validity)\n",
    "        return validity\n",
    "\n",
    "\n",
    "def sample_z(batch_size = 1, d_noise=100):\n",
    "    return torch.randn(batch_size, d_noise, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "d_noise = 100\n",
    "d_model = 256 #논문에서는 512차원\n",
    "nhead = 8 #Q,K,V vector 256 / 8 = 32차원으로 축소, multihead attention 8번\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = TransformerGenerator(d_noise, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)\n",
    "discriminator = TransformerDiscriminator(d_model, nhead, num_encoder_layers, dim_feedforward, dropout).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "z = sample_z()\n",
    "img_fake = generator(z).view(-1,28,28)\n",
    "#이미지 출력\n",
    "imshow(img_fake.squ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "batch_size = 64\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train Discriminator\n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    # Sample noise as generator input\n",
    "    z = torch.randn(batch_size, d_noise).to(device)\n",
    "    \n",
    "    # Generate a batch of images\n",
    "    gen_imgs = generator(z)\n",
    "    \n",
    "    # Generate a batch of real images\n",
    "    real_imgs = torch.randn(batch_size, 1, 28, 28).to(device)  # Replace with real images from your dataset\n",
    "    \n",
    "    # Adversarial ground truths\n",
    "    valid = torch.ones(batch_size, 1).to(device)\n",
    "    fake = torch.zeros(batch_size, 1).to(device)\n",
    "    \n",
    "    # Loss for real images\n",
    "    real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "    \n",
    "    # Loss for fake images\n",
    "    fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "    \n",
    "    # Total discriminator loss\n",
    "    d_loss = (real_loss + fake_loss) / 2\n",
    "    \n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    # Train Generator\n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    # Loss for fake images\n",
    "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "    \n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} - D loss: {d_loss.item()} - G loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(d_noise, d_hidden).to(device)\n",
    "discriminator = TransformerDiscriminator(d_model, nhead, num_encoder_layers, dim_feedforward, dropout).to(device)\n",
    "\n",
    "# The rest of the training loop remains the same\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
